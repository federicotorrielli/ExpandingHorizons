{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade55bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!df --output=source,pcent,avail -BG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19304f-7461-43ef-80b4-d8b48914842c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:42:19.038977Z",
     "iopub.status.busy": "2023-10-18T12:42:19.038058Z",
     "iopub.status.idle": "2023-10-18T12:42:22.514569Z",
     "shell.execute_reply": "2023-10-18T12:42:22.513822Z",
     "shell.execute_reply.started": "2023-10-18T12:42:19.038947Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U vllm ujson tqdm\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea4cdd-34ae-4bc8-80b5-1719ece0cb95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:44:31.909335Z",
     "iopub.status.busy": "2023-10-18T12:44:31.908329Z",
     "iopub.status.idle": "2023-10-18T12:44:31.913186Z",
     "shell.execute_reply": "2023-10-18T12:44:31.912696Z",
     "shell.execute_reply.started": "2023-10-18T12:44:31.909295Z"
    }
   },
   "outputs": [],
   "source": [
    "import ujson\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor.parallel_utils.parallel_state import destroy_model_parallel\n",
    "\n",
    "model_names = [\n",
    "                # \"tiiuae/falcon-40b\",\n",
    "                \"BAAI/Aquila2-7B\",\n",
    "                \"databricks/dolly-v2-12b\",\n",
    "                \"mistralai/Mistral-7B-v0.1\",\n",
    "                \"meta-llama/Llama-2-13b-hf\",\n",
    "                \"lmsys/vicuna-13b-v1.5\",\n",
    "                \"mosaicml/mpt-30b-instruct\",\n",
    "               ]\n",
    "\n",
    "sampling_params = SamplingParams(top_p=0.9, temperature=0.35, max_tokens=50)\n",
    "\n",
    "t = sampling_params.temperature\n",
    "top_p = sampling_params.top_p\n",
    "max_new_tokens = sampling_params.max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109c7f6-8e22-4264-9cd4-c019e719951d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:42:43.798374Z",
     "iopub.status.busy": "2023-10-18T12:42:43.797751Z",
     "iopub.status.idle": "2023-10-18T12:42:43.811908Z",
     "shell.execute_reply": "2023-10-18T12:42:43.811009Z",
     "shell.execute_reply.started": "2023-10-18T12:42:43.798344Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_number_prompts(file):\n",
    "  with open(file, 'r') as reader:\n",
    "    all_data = ujson.load(reader)\n",
    "    counter = 0\n",
    "    for _ in all_data:\n",
    "      counter += 1\n",
    "    return counter\n",
    "\n",
    "def prompt_generator(file):\n",
    "  with open(file, 'r') as reader:\n",
    "    all_data = ujson.load(reader)\n",
    "    for data in all_data:\n",
    "      yield data[\"cat\"], data[\"slot\"], data[\"value\"], data[\"prompt\"]\n",
    "      \n",
    "def get_only_prompts(file):\n",
    "  with open(file, 'r') as reader:\n",
    "    all_data = ujson.load(reader)\n",
    "    output = []\n",
    "    for data in all_data:\n",
    "      output.append(data[\"prompt\"])\n",
    "    return output\n",
    "\n",
    "def get_all_prompts(file):\n",
    "  with open(file, 'r') as reader:\n",
    "    all_data = ujson.load(reader)\n",
    "    output = []\n",
    "    for data in all_data:\n",
    "      output.append((data[\"cat\"], data[\"slot\"], data[\"value\"], data[\"prompt\"]))\n",
    "    return output\n",
    "\n",
    "def get_simple_model_name(m_name):\n",
    "  if '/' in m_name:\n",
    "    m_name = m_name.split('/')[-1]\n",
    "  return m_name\n",
    "\n",
    "input_f = 'one_shot_prompt_v1.json'\n",
    "!wget \"https://evilscript.eu/upload/files/one_shot_prompt_v1.json\"\n",
    "num_prompts = get_number_prompts(input_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64765a09-9c88-478d-b261-697f90d00f10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:44:34.450226Z",
     "iopub.status.busy": "2023-10-18T12:44:34.449539Z",
     "iopub.status.idle": "2023-10-18T12:44:35.379992Z",
     "shell.execute_reply": "2023-10-18T12:44:35.376544Z",
     "shell.execute_reply.started": "2023-10-18T12:44:34.450191Z"
    }
   },
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "  f_m_name = get_simple_model_name(model_name)\n",
    "  file_output = f'{f_m_name}__t_{t}__top_p_{top_p}__max_new_tokens_{max_new_tokens}.jsonl'\n",
    "  \n",
    "  print(f'Running {model_name} with {num_prompts} prompts')\n",
    "  \n",
    "  model = LLM(model_name, trust_remote_code=True, tensor_parallel_size=torch.cuda.device_count())\n",
    "  \n",
    "  with open(file_output, 'w') as writer:\n",
    "    generator = prompt_generator(input_f)\n",
    "    full_list_prompts = get_only_prompts(input_f)\n",
    "    model_outputs = model.generate(full_list_prompts, sampling_params=sampling_params)\n",
    "    outputs_dict = {}\n",
    "    for output in model_outputs:\n",
    "      prompt = output.prompt\n",
    "      generated_text = output.outputs[0].text\n",
    "      outputs_dict[prompt] = generated_text\n",
    "    for (cat, slot, value, prompt) in tqdm(generator, total=num_prompts):\n",
    "      model_output = outputs_dict[prompt]\n",
    "      json_dump = ujson.dumps({\"cat\": cat,\n",
    "                              \"slot\": slot,\n",
    "                              \"value\": value,\n",
    "                              \"prompt\": prompt,\n",
    "                              \"result\": model_output})\n",
    "      writer.write(json_dump + '\\n')\n",
    "    with torch.no_grad():\n",
    "        destroy_model_parallel()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb57a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip all the produced files, then remove them\n",
    "!zip -r produced_files.zip *.jsonl\n",
    "!rm *.jsonl\n",
    "# upload the files\n",
    "!curl --upload-file ./produced_files.zip https://transfer.sh/produced_files.zip\n",
    "# remove the zip file\n",
    "!rm produced_files.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
